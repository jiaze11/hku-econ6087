\documentclass{beamer}
\usetheme{metropolis}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{ctex}

% Setup code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\title{Text Classification for Economics}
\subtitle{A Practical Introduction to Python \& Machine Learning}
\author{HKU ECON6087 Tutorial}
\date{\today}

\begin{document}

% Title Page
\frame{\titlepage}

% Objective
\begin{frame}{Objective}
    \begin{itemize}
        \item \textbf{Goal}: Classify news articles into categories (World, Sports, Business, Sci/Tech).
        \item \textbf{Tools}: Python and its data science libraries.
        \item \textbf{Method}: K-Nearest Neighbors (KNN).
        \item \textbf{Key Learning}: How specific text representations (Bag of Words vs. TF-IDF) affect model performance.
    \end{itemize}
\end{frame}

% Python Modules Explained
\section{Python Libraries}
\begin{frame}{The Python Data Stack}
    We use three main libraries—think of them as your digital toolbox:
    
    \begin{description}
        \item[datasets] \textbf{(Hugging Face)}: Like a library catalog. It lets us easily download and access standard datasets without manual searching.
        \item[pandas] \textbf{(Data Manipulation)}: The "Excel of Python". It handles data tables (DataFrames), allowing us to load, view, and clean data efficiently.
        \item[scikit-learn] \textbf{(Machine Learning)}: The standard toolkit for machine learning. It contains the algorithms (like KNN) and tools to transform text into numbers.
    \end{description}
\end{frame}

% The Data
\section{The Data}
\begin{frame}{The Dataset: AG News}
    We use the \textbf{AG News} dataset used in the tutorial.
    \vspace{0.5cm}
    \begin{itemize}
        \item \textbf{Content}: News headlines and short descriptions.
        \item \textbf{Classes}: 4 Topics
        \begin{enumerate}
            \item World
            \item Sports
            \item Business
            \item Sci/Tech
        \end{enumerate}
        \item \textbf{Size}:
        \begin{itemize}
            \item Training Set: 120,000 articles (used to teach the model).
            \item Testing Set: 7,600 articles (used to evaluate performance).
        \end{itemize}
    \end{itemize}
\end{frame}

% Step 1: Loading Data
\section{Step 1: Loading Data}
\begin{frame}[fragile]{Step 1: Loading and Saving Data}
    We first load the data from the cloud and save it locally as CSV files.
    \begin{lstlisting}
from datasets import load_dataset

# 1. Download data
dataset = load_dataset("ag_news")

# 2. Save as CSV (for Pandas to read later)
for split in dataset.keys():
    # dataset[split] is the data (train or test)
    # .to_csv() saves it to a file
    dataset[split].to_csv(f"{split}.csv")
    \end{lstlisting}
    \vspace{0.2cm}
    \textit{Result: We now have `train.csv` and `test.csv`.}
\end{frame}

% Concept: KNN
\section{Concept: K-Nearest Neighbors}
\begin{frame}{Concept: K-Nearest Neighbors (KNN)}
    \begin{block}{Intuition}
        "Tell me who your neighbors are, and I'll tell you who you are."
    \end{block}
    
    To classify a new document:
    \begin{enumerate}
        \item We place the new document in the "space" of all known documents.
        \item We find the \textbf{K} closest documents (neighbors).
        \item We look at their labels (topics).
        \item We assign the majority label to the new document.
    \end{enumerate}
    
    \vspace{0.3cm}
    \textbf{Challenge for Text}: How do we measure distance between text? We must turn words into numbers.
\end{frame}

% Step 2: Bag of Words
\section{Step 2: Bag of Words}
\begin{frame}{Text Representation 1: Bag of Words (BoW)}
    The simplest way to turn text into numbers.
    \begin{itemize}
        \item We list all unique words in the dataset (the vocabulary).
        \item For each document, we simply \textbf{count} how many times each word appears.
    \end{itemize}
    
    \begin{example}
        "The economy grows" $\rightarrow$ \{'the': 1, 'economy': 1, 'grows': 1, 'football': 0\}
    \end{example}
    
    \textbf{Pros}: Simple. \\
    \textbf{Cons}: Ignores grammar. Common words like "the" appear frequently but have little meaning.
\end{frame}

\begin{frame}[fragile]{Implementing Bag of Words}
    In Python, \texttt{CountVectorizer} creates the Bag of Words matrix.
    \begin{lstlisting}
from sklearn.feature_extraction.text import CountVectorizer

# Create a "counter" that looks at the top 5000 words
# stop_words='english' removes common words like "the", "is", "at"
vectorizer = CountVectorizer(stop_words='english', max_features=5000)

# Learn the vocabulary and count words in training data
X_train_bow = vectorizer.fit_transform(train_df['text'])
# Count words in test data (using same vocabulary)
X_test_bow = vectorizer.transform(test_df['text'])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Results: KNN with Bag of Words}
    \begin{lstlisting}
from sklearn.neighbors import KNeighborsClassifier

# Create the model (look at 5 nearest neighbors)
knn = KNeighborsClassifier(n_neighbors=5)

# Train: Model memorizes the training data
knn.fit(X_train_bow, train_df['label'])

# Predict and Evaluate
accuracy = knn.score(X_test_bow, test_df['label'])
print(f"Accuracy: {accuracy}") 
    \end{lstlisting}
    \begin{alertblock}{Result}
        Accuracy $\approx$ 72.5\%
    \end{alertblock}
\end{frame}

% Step 3: TF-IDF
\section{Step 3: TF-IDF}
\begin{frame}{Text Representation 2: TF-IDF}
    \textbf{T}erm \textbf{F}requency - \textbf{I}nverse \textbf{D}ocument \textbf{F}requency.
    \vspace{0.3cm}
    
    A smarter way to count.
    \begin{itemize}
        \item \textbf{TF (Term Frequency)}: How often word $w$ appears in document $d$.
        \item \textbf{IDF (Inverse Document Frequency)}: How rare is word $w$ across \textit{all} documents?
    \end{itemize}
    
    \textbf{Idea}: If a word appears in \textit{every} document (e.g., "said", "today"), it's not useful for classification. We lower its weight. If a word is rare (e.g., "touchdown", "deficit"), it gets a high weight.
\end{frame}

\begin{frame}[fragile]{Implementing TF-IDF}
    Changing the code is easy: swap \texttt{CountVectorizer} for \texttt{TfidfVectorizer}.
    \begin{lstlisting}
from sklearn.feature_extraction.text import TfidfVectorizer

# Use TF-IDF instead of simple counts
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)

# Transform data
X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])
X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Results: KNN with TF-IDF}
    We train the exact same KNN model, just using the new weighted data.
    \begin{lstlisting}
# Train KNN on TF-IDF data
knn_tfidf = KNeighborsClassifier(n_neighbors=5)
knn_tfidf.fit(X_train_tfidf, train_df['label'])

# Evaluate
accuracy_tfidf = knn_tfidf.score(X_test_tfidf, test_df['label'])
print(f"Accuracy: {accuracy_tfidf}")
    \end{lstlisting}
    \begin{alertblock}{Result}
        Accuracy $\approx$ 89.0\%
    \end{alertblock}
    \textbf{Takeaway}: Weighting words by importance (TF-IDF) massively improves our simple neighbor-based classifier.
\end{frame}

% Extension: Chinese NLP
\section{Extension: Chinese Text}
\begin{frame}{Working with Chinese Data}
    \textbf{Challenge}: English words are separated by spaces. Chinese words are not.
    \begin{itemize}
        \item English: "I love economics" $\rightarrow$ ["I", "love", "economics"]
        \item Chinese: "我爱经济学" $\rightarrow$ ?
    \end{itemize}
    \vspace{0.5cm}
    \textbf{Solution}: Word Segmentation (Tokenization).
    \begin{itemize}
        \item We use a library called \textbf{\texttt{jieba}} ("Stutter" in Chinese) to cut sentences into words.
        \item "我爱经济学" $\xrightarrow{jieba}$ ["我", "爱", "经济学"]
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Chinese Segmentation with Jieba}
    \begin{lstlisting}
import jieba

# Example Function
def segment_text(text):
    # jieba.cut returns a generator, we join it with spaces
    return " ".join(jieba.cut(text))

# Apply to a sentence
print(segment_text("我爱经济学"))
# Output: "我 爱 经济学"
    \end{lstlisting}
    
    Once the text is space-separated, we can use \texttt{CountVectorizer} and \texttt{TfidfVectorizer} exactly as before!
\end{frame}

% Conclusion
\section{Conclusion}
\begin{frame}{Summary}
    \begin{enumerate}
        \item \textbf{Pandas} allows economists to handle large datasets easily.
        \item \textbf{Scikit-Learn} provides plug-and-play machine learning tools.
        \item Text must be converted to numbers before analysis.
        \item \textbf{TF-IDF} is often superior to simple counting because it filters out "noise" and highlights unique keywords.
        \item Even a simple algorithm like \textbf{KNN} can achieve high accuracy ($\sim$89\%) with the right data representation.
    \end{enumerate}
\end{frame}

\end{document}
