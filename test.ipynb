{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748bb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "# Combine the dataset and save it to a CSV file\n",
    "import pandas as pd\n",
    "df_1 = pd.DataFrame(dataset['train'])\n",
    "df_2 = pd.DataFrame(dataset['test'])\n",
    "df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "df.to_csv('ag_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b910855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c04f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed2df3b91ab4bf6bba8e626b6e23441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ce5b15ecf3490d85739996a84e8622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset[split].to_csv(f\"{split}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5811f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (120000, 2)\n",
      "Test shape: (7600, 2)\n",
      "                                                text  label\n",
      "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
      "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
      "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
      "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
      "4  Oil prices soar to all-time record, posing new...      2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2643243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix (Train): (120000, 5000)\n",
      "Bag of Words Matrix (Test): (7600, 5000)\n",
      "Some feature names: ['00' '000' '04' '05' '10' '100' '101' '10th' '11' '11th' '12' '120'\n",
      " '12th' '13' '14' '146' '15' '150' '151' '16']\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_bow = vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "# Transform the test data\n",
    "X_test_bow = vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(\"Bag of Words Matrix (Train):\", X_train_bow.shape)\n",
    "print(\"Bag of Words Matrix (Test):\", X_test_bow.shape)\n",
    "\n",
    "# Show some feature names (words)\n",
    "print(\"Some feature names:\", vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de3cac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier...\n",
      "Predicting on test data...\n",
      "KNN Accuracy: 0.7249\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the classifier\n",
    "print(\"Training KNN classifier...\")\n",
    "knn.fit(X_train_bow, train_df['label'])\n",
    "\n",
    "# Predict on test data\n",
    "print(\"Predicting on test data...\")\n",
    "y_pred = knn.predict(X_test_bow)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['label'], y_pred)\n",
    "print(f\"KNN Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833280f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (Train): (120000, 5000)\n",
      "TF-IDF Matrix (Test): (7600, 5000)\n",
      "Training KNN classifier on TF-IDF data...\n",
      "Predicting on test data (TF-IDF)...\n",
      "KNN Accuracy (TF-IDF): 0.8901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(\"TF-IDF Matrix (Train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF Matrix (Test):\", X_test_tfidf.shape)\n",
    "\n",
    "# Train the KNN classifier on TF-IDF data\n",
    "print(\"Training KNN classifier on TF-IDF data...\")\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_tfidf.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "# Predict on test data\n",
    "print(\"Predicting on test data (TF-IDF)...\")\n",
    "y_pred_tfidf = knn_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_tfidf = accuracy_score(test_df['label'], y_pred_tfidf)\n",
    "print(f\"KNN Accuracy (TF-IDF): {accuracy_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5056705",
   "metadata": {},
   "source": [
    "## Chinese Text Classification (TNEWS)\n",
    "\n",
    "We will now perform the same exercise on Chinese news titles from the CLUE benchmark (TNEWS dataset).\n",
    "\n",
    "**Key differences:**\n",
    "1. We need to use **jieba** to segment Chinese characters into words (tokens) because Chinese text doesn't use spaces.\n",
    "2. We will use the `validation` set as our test set, because the official `test` set labels are often hidden for fair benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a14b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = load_dataset(\"clue\", \"tnews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c3285e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 53360\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df84112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f1df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TNEWS dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaze/hku-econ6087/.venv/lib/python3.13/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 53360\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "First example: {'sentence': '上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？', 'label': 7, 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load TNEWS dataset\n",
    "# We use the CLUE benchmark version of TNEWS\n",
    "print(\"Loading TNEWS dataset...\")\n",
    "c_dataset = load_dataset(\"clue\", \"tnews\")\n",
    "\n",
    "# Display dataset structure\n",
    "print(c_dataset)\n",
    "print(\"First example:\", c_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f73cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting text (this may take a few seconds)...\n",
      "Original: 上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？\n",
      "Segmented: 上课时 学生 手机 响个 不停 ， 老师 一怒之下 把 手机 摔 了 ， 家长 拿 发票 让 老师 赔 ， 大家 怎么 看待 这种 事 ？\n"
     ]
    }
   ],
   "source": [
    "# Function to segment Chinese text using jieba\n",
    "def segment_text(examples):\n",
    "    # 'sentence' is the column name in TNEWS containing the text\n",
    "    return {\"text_cut\": [\" \".join(jieba.cut(text)) for text in examples[\"sentence\"]]}\n",
    "\n",
    "# Apply segmentation to all splits\n",
    "print(\"Segmenting text (this may take a few seconds)...\")\n",
    "c_dataset = c_dataset.map(segment_text, batched=True)\n",
    "\n",
    "print(\"Original:\", c_dataset['train'][0]['sentence'])\n",
    "print(\"Segmented:\", c_dataset['train'][0]['text_cut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0dff29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easier machine learning workflow\n",
    "train_df_c = c_dataset['train'].to_pandas()\n",
    "test_df_c = c_dataset['validation'].to_pandas() # Using validation as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db87073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Chinese text (Bag of Words)...\n",
      "Training KNN on Chinese data (BoW)...\n",
      "Predicting (BoW)...\n",
      "Chinese News Classification Accuracy (KNN+BoW): 0.2599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "# Limit max_features to 5000 as well\n",
    "count_vectorizer_c = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform\n",
    "print(\"Vectorizing Chinese text (Bag of Words)...\")\n",
    "X_train_bow_c = count_vectorizer_c.fit_transform(train_df_c['text_cut'])\n",
    "X_test_bow_c = count_vectorizer_c.transform(test_df_c['text_cut'])\n",
    "\n",
    "# Train KNN\n",
    "print(\"Training KNN on Chinese data (BoW)...\")\n",
    "knn_bow_c = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_bow_c.fit(X_train_bow_c, train_df_c['label'])\n",
    "\n",
    "# Evaluate\n",
    "print(\"Predicting (BoW)...\")\n",
    "y_pred_bow_c = knn_bow_c.predict(X_test_bow_c)\n",
    "accuracy_bow_c = accuracy_score(test_df_c['label'], y_pred_bow_c)\n",
    "\n",
    "print(f\"Chinese News Classification Accuracy (KNN+BoW): {accuracy_bow_c:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456fd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Chinese text (TF-IDF)...\n",
      "Training KNN on Chinese data (TF-IDF)...\n",
      "Predicting (TF-IDF)...\n",
      "Chinese News Classification Accuracy (KNN+TFIDF): 0.2518\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "# Note: We don't need stop_words='english' obviously.\n",
    "# We can provide a chinese stop_words list, but we'll skip it for simplicity here.\n",
    "tfidf_vectorizer_c = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform\n",
    "print(\"Vectorizing Chinese text (TF-IDF)...\")\n",
    "X_train_c = tfidf_vectorizer_c.fit_transform(train_df_c['text_cut'])\n",
    "X_test_c = tfidf_vectorizer_c.transform(test_df_c['text_cut'])\n",
    "\n",
    "# Train KNN\n",
    "print(\"Training KNN on Chinese data (TF-IDF)...\")\n",
    "knn_c = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_c.fit(X_train_c, train_df_c['label'])\n",
    "\n",
    "# Evaluate\n",
    "print(\"Predicting (TF-IDF)...\")\n",
    "y_pred_c = knn_c.predict(X_test_c)\n",
    "accuracy_c = accuracy_score(test_df_c['label'], y_pred_c)\n",
    "\n",
    "print(f\"Chinese News Classification Accuracy (KNN+TFIDF): {accuracy_c:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hku-econ6087",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
