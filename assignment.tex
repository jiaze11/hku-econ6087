\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
% \usepackage{ctex} % Uncomment if your LaTeX environment supports it for rendering Chinese characters

\geometry{a4paper, margin=1in}

\title{ECON6087: Assignment 1 \\ Chinese Text Classification with Machine Learning}
\author{Machine Learning for Economists}
\date{Due Date: [Insert Date]}

\begin{document}

\maketitle

\section*{Introduction}
In the tutorial, we classified English news using the AG News dataset. In this assignment, you will apply the same techniques to a Chinese dataset: \textbf{TNEWS} (Toutiao News) from the CLUE benchmark. You will handle the unique challenge of Chinese word segmentation using the \texttt{jieba} library.

\section*{Requirements}
\begin{itemize}
    \item You must use Python for this assignment.
    \item Required libraries: \texttt{pandas}, \texttt{scikit-learn}, \texttt{datasets}, \texttt{jieba}.
    \item Submit your code as a Jupyter Notebook (.ipynb) or a Python script (.py).
\end{itemize}

\section*{Task 1: Data Preparation}
\begin{enumerate}
    \item Load the \textbf{TNEWS} dataset using the command: \texttt{load\_dataset("clue", "tnews")}.
    \item Convert the \texttt{'train'} and \texttt{'validation'} splits into pandas DataFrames. (We use the validation set as our test set for this assignment).
    \item Inspect the data. Display the first 5 rows of the training dataframe.
\end{enumerate}

\section*{Task 2: Chinese Segmentation with Jieba}
Since Chinese text does not use spaces to separate words, you must segment the text before vectorization.
\begin{enumerate}
    \item Import the \texttt{jieba} library.
    \item Create a function that takes a Chinese string and returns a space-separated string of words using \texttt{jieba.cut}.
    \item Apply this function to the text column of both your training and testing (validation) dataframes.
    \item Create a new column (e.g., \texttt{text\_cut}) to store these segmented strings.
\end{enumerate}

\section*{Task 3: Bag of Words Classification}
\begin{enumerate}
    \item Use \texttt{CountVectorizer} to convert the segmented text (\texttt{text\_cut}) into a numerical matrix.
    \item Limit the vocabulary size to the top \textbf{5,000} features.
    \item Train a \textbf{K-Nearest Neighbors (KNN)} classifier ($k=5$) on the training data.
    \item Report the classification accuracy on the test set.
\end{enumerate}

\section*{Task 4: TF-IDF Classification}
\begin{enumerate}
    \item Use \texttt{TfidfVectorizer} to convert the segmented text (\texttt{text\_cut}) into a numerical matrix.
    \item Keep the vocabulary limit at \textbf{5,000} features.
    \item Train a \textbf{K-Nearest Neighbors (KNN)} classifier ($k=5$) on the training data.
    \item Report the classification accuracy on the test set.
\end{enumerate}

\section*{Task 5: Comparison and Analysis}
\begin{itemize}
    \item Compare the accuracy of BoW vs. TF-IDF. Which one performed better?
    \item Explain why one method might be better suited for short news headlines (TNEWS) than the other.
\end{itemize}

\end{document}
