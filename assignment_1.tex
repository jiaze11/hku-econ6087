\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{xcolor}

% Course Details
\title{\textbf{ECON6087: Textual Analysis for Economists} \\ Assignment 1: Bag of Words and TF-IDF}
\author{}
\date{Spring 2026}

\begin{document}

\maketitle

\section*{General Instructions}
\begin{itemize}
    \item \textbf{Submission:} Please submit a short report and your source code. 
    \begin{itemize}
        \item \textbf{Report Format:} Accepted formats include \texttt{.md}, \texttt{.doc}, \texttt{.docx}, \texttt{.pdf}, or embedded directly within a Jupyter Notebook (\texttt{.ipynb}).
        \item \textbf{Code:} You may use any programming language.
    \end{itemize}
    \item \textbf{File Format:} This assignment uses \textbf{.parquet} files. Ensure your environment has the necessary dependencies installed to read this format (e.g., \texttt{pyarrow} or \texttt{fastparquet}).
    \item \textbf{Reproducibility:} Ensure your code is well-commented. If your algorithms involve randomization (e.g., classifier initialization), set a fixed \textbf{random seed} to ensure your results are reproducible.
\end{itemize}

\vspace{1em}

\section*{Question 1: Text Vectorization and Supervised Learning (English)}

In this exercise, you will work with the \texttt{ag\_news} dataset, a collection of news articles categorized into four topics: World (0), Sports (1), Business (2), and Sci/Tech (3).

\subsection*{1.1 Data Preparation}
Download the following files from Moodle:
\begin{itemize}
    \item \texttt{ag\_news\_train.parquet}: The training dataset.
    \item \texttt{ag\_news\_test.parquet}: The testing dataset.
\end{itemize}

Both files contain two primary columns: \texttt{text} (the news snippet) and \texttt{label} (the category index).

\subsection*{1.2 Methodology}
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Preprocessing:} 
    Load the training and testing files. Perform basic text preprocessing on the \texttt{text} column for both datasets:
    \begin{itemize}
        \item Convert text to lowercase.
        \item Remove punctuation and special characters.
        \item Remove standard English stop words.
    \end{itemize}
    
    \item \textbf{Vectorization:} Implement two methods to transform the raw text into numerical feature vectors. You may use standard libraries (e.g., \texttt{scikit-learn}).
    \begin{itemize}
        \item \textbf{Bag of Words (BoW):} Represent each document by the count of words occurring in it.
        \item \textbf{TF-IDF:} Represent each document using Term Frequency-Inverse Document Frequency weights.
    \end{itemize}
    \textit{Note: Fit your vectorizers (i.e., build the vocabulary) using only the \textbf{training} set, then transform both the training and testing sets.}

    \item \textbf{Modeling:} Train a classification model to predict the news category using the vectors generated in step (b). You may use \textbf{K-Nearest Neighbors (KNN)} or another standard classifier (e.g., Logistic Regression, Naive Bayes).
\end{enumerate}

\subsection*{1.3 Reporting Requirements}
For both vectorization methods (BoW and TF-IDF), report the following:
\begin{enumerate}
    \item \textbf{Dictionary Size:} The total number of unique tokens (features) in your vocabulary derived from the training set.
    \item \textbf{Top 10 Words:} A list of the 10 most frequent words found in the training corpus.
    \item \textbf{Model Performance:} Report the classification accuracy on both the \textbf{Training Set} and the \textbf{Testing Set}.
\end{enumerate}

\newpage

\section*{Question 2: Textual Analysis with Chinese Data}

You will now apply the classification techniques from Question 1 to a Chinese-language dataset: \texttt{tnews}, which is part of the CLUE benchmark. You can find more details about the dataset structure at the Hugging Face repository: \url{https://huggingface.co/datasets/clue/clue}.

\subsection*{2.1 Data Preparation}
Download the following files from Moodle:
\begin{itemize}
    \item \texttt{tnews\_train.parquet}: The training dataset.
    \item \texttt{tnews\_test.parquet}: The testing dataset.
\end{itemize}

The files contain:
\begin{itemize}
    \item \texttt{sentence}: The news title text.
    \item \texttt{label}: The category of the news.
\end{itemize}

\subsection*{2.2 Methodology}
\begin{enumerate}[label=(\alph*)]
    \item \textbf{Preprocessing (Segmentation):} 
    Since Chinese text is not delimited by spaces, use a specialized library (e.g., \texttt{jieba}) to segment the continuous strings of characters in the \texttt{sentence} column into meaningful tokens. Apply this to both training and testing sets.
    \begin{itemize}
        \item Segment the text.
        \item Remove stop words (using a Chinese-specific list like Baidu or HIT) and punctuation.
    \end{itemize}
    
    \item \textbf{Vectorization:} Implement two methods to transform the segmented text into numerical feature vectors.
    \begin{itemize}
        \item \textbf{Bag of Words (BoW)}
        \item \textbf{TF-IDF}
    \end{itemize}
    \textit{Note: Fit your vectorizers using only the \textbf{training} set, then transform both the training and testing sets.}

    \item \textbf{Modeling:} Train a classification model to predict the news category using the vectors generated in step (b). Use the same classification algorithm chosen in Question 1.
\end{enumerate}

\subsection*{2.3 Reporting Requirements}
For both vectorization methods (BoW and TF-IDF), report the following:
\begin{enumerate}
    \item \textbf{Dictionary Size:} The total number of unique Chinese tokens in your vocabulary after segmentation and filtering.
    \item \textbf{Top 10 Words:} A list of the 10 most frequent Chinese words in the training set.
    \item \textbf{Model Performance:} Report the classification accuracy on both the \textbf{Training Set} and the \textbf{Testing Set}.
\end{enumerate}

\vspace{1em}
\textit{Technical Note: Ensure your coding environment supports UTF-8 encoding to correctly process and display Chinese characters.}

\end{document}